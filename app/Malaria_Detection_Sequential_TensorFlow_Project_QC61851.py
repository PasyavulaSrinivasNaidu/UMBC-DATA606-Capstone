# -*- coding: utf-8 -*-
"""Final_Malaria_Detection_QC61851.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kQzFczG8TA0BkT0bZLoZ6Fl2OcbAWH-F

# **`Project Title: MalariaNet - Deep Learning for Malaria Image Classification with TensorFlow`**

#### UMBC Data Science Master Degree Capstone - DATA606

**Guided by:**

Dr. Chaojie (Jay) Wang

### **Data:**
### **Data Source:** [Kaggle - Malaria Cell Images Dataset](https://www.kaggle.com/datasets/iarunava/cell-images-for-detecting-malaria/data)
The dataset "Cell Images for Detecting Malaria" contains a collection of images representing blood smears of individuals infected with malaria and uninfected individuals.

### **Author Information:**
* **Name** - Srinivas Naidu Pasyavula

* **University** ID - QC61851
* **Github** - [Srinivas Naidu Pasyavula - Github](https://github.com/PasyavulaSrinivasNaidu/UMBC-DATA606-Capstone/)
* **LinkedIn** - [Srinivas Naidu Pasyavula - LinkedIn](https://www.linkedin.com/in/srinivas-naidu-pasyavula/)
* **Powerpoint Presentation** - [Project Presentation File]()
* **Youtube Video** - [Project Presentation Video]()

References - [Neuralearn](https://github.com/Neuralearn/deep-learning-with-tensorflow-2/blob/main/deep%20learning%20for%20computer%20vision/2-Malaria%20Detection%20by%20Neuralearn.ai-.ipynb)

---

# **`Creating TensorFlow Keras Model`**

## **`1. Importing all the Required Libraries`**
"""

import tensorflow as tf### models
import numpy as np### math computations
import matplotlib.pyplot as plt### plots
import sklearn### machine learning library
import cv2## image processing
from sklearn.metrics import confusion_matrix, roc_curve### metrics
import seaborn as sns### visualizations
import datetime
import io
import os
import random
from google.colab import files
import PIL
from PIL import Image
import albumentations as A
import tensorflow_datasets as tfds
import tensorflow_probability as tfp
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, InputLayer, BatchNormalization, Input, Dropout, RandomFlip, RandomRotation, Resizing, Rescaling
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import BinaryAccuracy, FalsePositives, FalseNegatives, TruePositives, TrueNegatives, Precision, Recall, AUC, binary_accuracy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.regularizers  import L2, L1
from tensorboard.plugins.hparams import api as hp
from google.colab import drive
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve

print("PIL Version = ",PIL.__version__)
print("Numpy Version = ", np.__version__)
print("TensorFlow Version = ", tf.__version__)

"""PIL Version =  10.3.0
Numpy Version =  1.26.4
TensorFlow Version =  2.16.1

## **`2. DATA PREPARATION`**

### **`2.1 Loading Dataset using Tensorflow datasets`**
"""

# Loading the dataset from the Tensorflow

dataset, dataset_info = tfds.load('malaria', with_info=True,
                                  as_supervised=True,
                                  shuffle_files = True,
                                  split=['train'])

"""### **`2.2 Data Exploration`**

**NOTE:** This dataset structure indicates that we have a dataset of images along with their corresponding labels. Each image is represented as a 3D tensor with variable height and width and 3 color channels (RGB), while each label is represented as a scalar integer.
"""

#This dataset structure indicates that, a dataset of images along with their corresponding labels. Each image is represented as a 3D tensor with variable height and width and 3 color channels (RGB), while each label is represented as a scalar integer.

dataset

#first 1 elements of the dataset and print each element. Each element of the dataset should be a tuple containing the image tensor and its corresponding label tensor

for data in dataset[0].take(1):
  print(data)

# Complete Dataset Information

dataset_info

"""### **`2.3 Setting up CONFIGURATION`**"""

CONFIGURATION = {
  "LEARNING_RATE": 0.001,
  "N_EPOCHS": 10,
  "BATCH_SIZE": 128,
  "DROPOUT_RATE": 0.0,
  "IM_SIZE": 224,
  "REGULARIZATION_RATE": 0.0,
  "N_FILTERS": 6,
  "KERNEL_SIZE": 3,
  "N_STRIDES": 1,
  "POOL_SIZE": 2,
  "N_DENSE_1": 100,
  "N_DENSE_2": 10,
}

"""## **`3. DATA Splitting`**

**NOTE -**

**The Dataset is splitted into three parts Training, Validation and Testing (TRAIN/VAL/TEST)**

**Percentage of data allocated for training/validation and testing**

TRAIN_RATIO - 80 %  

VAL_RATIO   - 10 %

TEST_RATIO  - 10 %
"""

# Creating Splits Function for splitting dataset into Train/Val/Test

def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):
    # Calculate the size of the dataset
    DATASET_SIZE = len(dataset)

    # Train dataset: Take a portion of the dataset for training based on TRAIN_RATIO
    train_dataset = dataset.take(int(TRAIN_RATIO * DATASET_SIZE))

    # Validation dataset: Skip the samples already taken for training, then take a portion for validation based on VAL_RATIO
    val_test_dataset = dataset.skip(int(TRAIN_RATIO * DATASET_SIZE))
    val_dataset = val_test_dataset.take(int(VAL_RATIO * DATASET_SIZE))

    # Test dataset: Skip the samples already taken for training and validation, the rest is for testing
    test_dataset = val_test_dataset.skip(int(VAL_RATIO * DATASET_SIZE))

    # Return the split datasets for training, validation, and testing
    return train_dataset, val_dataset, test_dataset

# Defining the Ratios for TRAIN/VAL/TEST data

TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1

train_dataset, val_dataset, test_dataset = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)

#print(list(train_dataset.take(1).as_numpy_iterator()),
#     list(val_dataset.take(1).as_numpy_iterator()),
#     list(test_dataset.take(1).as_numpy_iterator()))

print(train_dataset)
print(val_dataset)
print(test_dataset)

"""## **`4. Data Visualization`**"""

for i, (image, label) in enumerate(train_dataset.take(16)):
  ax = plt.subplot(4,4, i+1)
  plt.imshow(image)
  plt.title(dataset_info.features['label'].int2str(label), fontsize=10)
  plt.axis('off')
plt.tight_layout()
plt.show()

# labels of the dataset

label_0 = dataset_info.features['label'].int2str(0)
label_1 = dataset_info.features['label'].int2str(1)

print("Label 0:", label_0)
print("Label 1:", label_1)

"""## **`5.Data Pre-processing`**

### **`5.0 Augmentation Example`**
"""

def visualize(original, augmented):
  plt.subplot(1,2,1)
  plt.imshow(original)

  plt.subplot(1,2,2)
  plt.imshow(augmented)

original_image, label = next(iter(train_dataset))

augmented_image = tf.image.rot90(original_image)
augmented_image = tf.image.adjust_saturation(augmented_image, saturation_factor = 0.3)
augmented_image = tf.image.flip_left_right(augmented_image)

augmented_image = tf.image.resize(augmented_image, (224, 224))/255.0
visualize(original_image, augmented_image)

"""### **`5.1 Data Augmentation`**"""

@tf.function # Graph Mode
def resizing_rescale(image, label):
  return tf.image.resize(image, (CONFIGURATION['IM_SIZE'], CONFIGURATION['IM_SIZE']))/255.0, label

### Data Augmentation using tf.image

@tf.function # Graph Mode
def augment(image, label):

  # Resizing and rescaling before augmentation
  image, label = resizing_rescale(image,label)

  # Data Augmentation
  image = tf.image.rot90(image)
  image = tf.image.adjust_saturation(image, saturation_factor = 0.3)
  image = tf.image.flip_left_right(image)

  return image, label

print(train_dataset)
print(val_dataset)
print(test_dataset)

"""### **`5.2 Data Loading`**"""

# Test resizing_rescaling

test_dataset = test_dataset.map(resizing_rescale)

# Train Data augmentation using tf.image

BATCH_SIZE = 32  # Define the batch size for training

train_dataset = (
                  train_dataset
                  .shuffle(buffer_size=8, reshuffle_each_iteration=True)  # Shuffle the training dataset
                  .map(augment) # augmentation using tf.image
                  .batch(BATCH_SIZE)  # Batch the shuffled dataset
                  .prefetch(tf.data.AUTOTUNE)  # Prefetch batches to improve performance
)

# Validation Data

val_dataset = (
    val_dataset
    .shuffle(buffer_size=8, reshuffle_each_iteration=True)  # Shuffle the validation dataset
    .map(resizing_rescale)
    .batch(BATCH_SIZE)  # Batch the shuffled validation dataset
    .prefetch(tf.data.AUTOTUNE)  # Prefetch batches to improve performance
)

print(train_dataset)
print(val_dataset)
print(test_dataset)

"""## **`6. Model Creation and Training - Binary Classification`**

### **`6.1 Model Creation using - SEQUENTIAL API`**
"""

# Model Configurations

IM_SIZE = CONFIGURATION['IM_SIZE']
DROPOUT_RATE = CONFIGURATION['DROPOUT_RATE']
REGULARIZATION_RATE = CONFIGURATION['REGULARIZATION_RATE']
N_FILTERS = CONFIGURATION['N_FILTERS']
KERNEL_SIZE = CONFIGURATION['KERNEL_SIZE']
POOL_SIZE = CONFIGURATION['POOL_SIZE']
N_STRIDES = CONFIGURATION['N_STRIDES']

# LeNet Architecture Model

lenet_model = tf.keras.Sequential([
    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),

    Conv2D(filters = N_FILTERS , kernel_size = KERNEL_SIZE, strides = N_STRIDES , padding='valid', activation = 'relu',kernel_regularizer = L2(REGULARIZATION_RATE)),
    BatchNormalization(),
    MaxPool2D (pool_size = POOL_SIZE, strides= N_STRIDES*2),
    Dropout(rate = DROPOUT_RATE ),

    Conv2D(filters = N_FILTERS*2 + 4, kernel_size = KERNEL_SIZE, strides=N_STRIDES, padding='valid', activation = 'relu', kernel_regularizer = L2(REGULARIZATION_RATE)),
    BatchNormalization(),
    MaxPool2D (pool_size = POOL_SIZE, strides= N_STRIDES*2),

    Flatten(),

    Dense( CONFIGURATION['N_DENSE_1'], activation = "relu", kernel_regularizer = L2(REGULARIZATION_RATE)),
    BatchNormalization(),
    Dropout(rate = DROPOUT_RATE),

    Dense( CONFIGURATION['N_DENSE_2'], activation = "relu", kernel_regularizer = L2(REGULARIZATION_RATE)),
    BatchNormalization(),

    Dense(1, activation = "sigmoid"),

])

lenet_model.summary()

"""### **`6.2 Model Training`**"""

# Define the list of metrics
metrics = [
    TruePositives(name='tp'),
    FalsePositives(name='fp'),
    TrueNegatives(name='tn'),
    FalseNegatives(name='fn'),
    BinaryAccuracy(name='accuracy'),
    Precision(name='precision'),
    Recall(name='recall'),
    AUC(name='auc')
]

from keras.losses import BinaryCrossentropy

lenet_model.compile(
    optimizer=Adam(learning_rate=CONFIGURATION['LEARNING_RATE']),
    loss=BinaryCrossentropy(from_logits=False),
    metrics=metrics
)

history = lenet_model.fit(train_dataset,  validation_data=val_dataset,
                          epochs = CONFIGURATION['N_EPOCHS'], verbose =1,
                          )

"""## **`7. Plotting Model Loss and Accuracy`**"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train_loss', 'val_loss'])
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train_accuracy', 'val_accuracy'])
plt.show()

"""## **`8. Model Evaluation, Testing and Prediction`**

### **`8.1. Model Evaluation`**
"""

test_dataset = test_dataset.batch(1)

# Sequential API
lenet_model.evaluate(test_dataset)

"""### **`8.2. Model Testing`**"""

# Parasitized = 0 and UnInfected = 1

def parasite_or_not(x):
  if(x<0.5):
    return str('P')
  else:
    return str('U')

for i, (image, label) in enumerate(test_dataset.take(9)):
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(image[0])
  plt.title(str(parasite_or_not(label.numpy())) + ":" + str(parasite_or_not(lenet_model.predict(image)[0][0])))
  plt.axis('off')

"""### **`8.3. Model Prediction`**"""

lenet_model.predict(test_dataset.take(1))[0][0]

parasite_or_not(lenet_model.predict(test_dataset.take(1))[0][0])

"""## **`9. Visualization Confusion Matrix`**"""

labels = []
inp = []

for x, y in test_dataset.as_numpy_iterator():
  labels.append([y])
  inp.append(x)

print(np.array(inp).shape)
# Removing 1 from above
print(np.array(inp)[:,0,...].shape)

print(labels)

labels = np.array([i[0] for i in labels])
print(labels)

predicted = lenet_model.predict(inp)
print(predicted[:,0])

inp = np.expand_dims(inp, axis=0)
predicted = lenet_model.predict(inp)
print(predicted[:,0])

threshold = 0.5

cm = confusion_matrix(labels, predicted>threshold)
print(cm)

threshold = 0.5

cm = confusion_matrix(labels, predicted > threshold)
print(cm)
plt.figure(figsize=(8,8))

sns.heatmap(cm, annot=True,)
plt.title('Confusion matrix - {}'.format(threshold))
plt.ylabel('Actual')
plt.xlabel('Predicted')

"""## **`10. ROC Plots`**"""

fp, tp, thresholds = roc_curve(labels, predicted)
print(len(fp), len(tp), len(thresholds))

plt.plot(fp, tp)
plt.xlabel("False Positive rate")
plt.ylabel("True Positive rate")

plt.grid()

skip = 20
for i in range (0, len(thresholds), skip):
  plt.text(fp[i], tp[i], thresholds[i])

plt.show()

# Our Aim is to Reduce False Positives (Parasitised/Infected-0(Negative) ; UnInfected - 1 (Positive))
# The New threshold is pocked up at 0.69
threshold = 0.69

cm = confusion_matrix(labels, predicted>threshold)
print(cm)
plt.figure(figsize=(8,8))

sns.heatmap(cm, annot=True)
plt.title('Confusion Matrix - {}'.format(threshold))
plt.ylabel('Actual')
plt.xlabel('Predicted')

"""# **`Model Realtime Testing`**

**Note -** Giving Input Image to the Model
"""

cv2.imread('C1_thinF_IMG_20150604_104722_cell_9.png')

original_image = cv2.imread('C1_thinF_IMG_20150604_104722_cell_9.png')

print(f"Image shape: {original_image.shape}")

image = tf.image.resize(original_image, (224, 224))/255.0
visualize(original_image, image)

print(f"Image shape: {image.shape}")

image = tf.image.resize(original_image, (224, 224))/255.0
image = tf.expand_dims(image, axis=0)  # Add a new dimension at the beginning
#visualize(original_image, image)
print(f"Image shape: {image.shape}")
lenet_model.predict(image)

lenet_model.predict(image)

def parasite_or_not(x):
  if(x<0.5):
    return str('Parasitized')
  else:
    return str('UnInfected')

parasite_or_not(lenet_model.predict(image))

"""# **`Model Saving`**"""

# Save the model
lenet_model.save('malaria_detection_lenet_model_whole_1')
# Save the model in HDF5 format
lenet_model.save('malaria_detection_lenet_model.hdf5')
# Save the model in H5 format
lenet_model.save('malaria_detection_lenet_model.h5')

lenet_model.save_weights("malaria_detection_lenet_model_weights")